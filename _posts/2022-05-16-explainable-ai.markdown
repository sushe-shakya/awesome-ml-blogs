---
layout: posts
title: Don't just build your machine learning model, interpret it as well
---

**Goal:** Interpret the output of your machine learning model

**Problem statement:** Machine learning models are generally viewed as black box and it is often difficult to provide the reasoning for its observed output.

**Flow:**

- What are the available solutions ?
- Talk about [lime](https://github.com/marcotcr/lime) and [shap](https://github.com/slundberg/shap)
- Show applications